{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd #data processing\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train.shape, test.shape)\n",
    "print(\"Shape of Training data: \", train.shape)\n",
    "print(\"Shape of Testing data: \", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels  = train['label'].values.tolist()\n",
    "class_labels_set = set(class_labels)\n",
    "\n",
    "freq_list = []\n",
    "\n",
    "for c in class_labels_set:\n",
    "    freq_list.append(class_labels.count(c))\n",
    "\n",
    "print ('Freq',freq_list)\n",
    "print ('number',class_labels_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large = 22; med = 16; small = 12\n",
    "params = {'axes.titlesize': large,\n",
    "          'legend.fontsize': med,\n",
    "          'figure.figsize': (16, 10),\n",
    "          'axes.labelsize': med,\n",
    "          'axes.titlesize': med,\n",
    "          'xtick.labelsize': med,\n",
    "          'ytick.labelsize': med,\n",
    "          'figure.titlesize': large}\n",
    "plt.rcParams.update(params)\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the number of samples per class\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.countplot(x=\"label\", data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check number of NULL values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many null values in the dataset\n",
    "print(\"Null values in train data:\")\n",
    "print(train.isnull().sum())\n",
    "print('\\n\\n')\n",
    "\n",
    "print(\"Null values in test data:\")\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.isnull(),cmap='YlGnBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data imputation\n",
    "\n",
    "### _Datasets may have missing values, and this can cause problems for many machine learning algorithms. As such, it is good practice to identify and replace missing values for each column in your input data prior to modeling your prediction task. This is called missing data imputation, or imputing for short._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing the data\n",
    "test=test.fillna(' ') \n",
    "train=train.fillna(' ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.isnull(), cmap='YlGnBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_words(train['text'], 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df1 = pd.DataFrame(common_words, columns = ['text' , 'count'])\n",
    "df1.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n",
    "kind='bar', title='Top 20 words in dataset before removing stop words',color=['slateblue', 'blueviolet', 'violet', 'orchid', 'lightpink'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_bigram(train['text'], 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df3 = pd.DataFrame(common_words, columns = ['text' , 'count'])\n",
    "df3.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n",
    "kind='bar', title='Top 20 bigrams in dataset before removing stop words',color=['slateblue', 'blueviolet', 'violet', 'orchid', 'lightpink'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'].plot(\n",
    "    kind='hist',\n",
    "    bins=50,\n",
    "    title='Number of True vs Fake News')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the columns (title, author, text) into one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['total']=test['title']+' '+test['author']+test['text']\n",
    "train['total']=train['title']+' '+train['author']+train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuations from the String  \n",
    "sample = \"!</> NLP is $$ </>^sh!!!o%%rt &&%$fo@@@r^^^&&!& </>*Natural@# Language&&\\ Pro@@@##%^^&cessing!@# %%$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is gonna get selected we r gonna replace that with the empty string(2nd parameter)\n",
    "sample = re.sub(r'[^\\w\\s]','',sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "# nltk library is v important for text preprocessing and dealing with the textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading nltk data\n",
    "# nltk uses some data, punkt will hold some data related to tokenisation\n",
    "# different functions and techniques of nltk uses different data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The NLTK tokeniser has tokenised \\\"Computers are not as great at understanding words as they are numbers.\\\" into a list of tokens \", end=\"\\n\\n\")\n",
    "print(nltk.word_tokenize(\"Computers are not as great at understanding words as they are numbers.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Does this thing really work? Lets see.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Priniting all the different sentences in sample_text: \", end=\"\\n\\n\")\n",
    "for i in nltk.sent_tokenize(sample_text):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Priniting all the different words in sample_text: \", end=\"\\n\\n\")\n",
    "for i in nltk.word_tokenize(sample_text):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# corpus of nltk will hold the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=stopwords.words(\"english\")\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words = [w for w in words if not w in stop]\n",
    "#this is basically saying go through each word and add it into this new array only if it's not a part of the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in clean_words:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(sample_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words = [w for w in words if not w in stop]\n",
    "for i in clean_words:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stop + punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words = [w for w in words if not w in stop]\n",
    "clean_words\n",
    "#clean_words includes all the words in the sentence excluding the stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#Using WordNet lemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "input_str=\"Kites Babies Meeting Is Done Languages Cities Mice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the sentence\n",
    "input_str=nltk.word_tokenize(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these modules \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() #need to tokenise the complee sentence\n",
    "\n",
    "\n",
    "print(\"Below, see how kites->kite, babies->baby, languages -> language, cities -> city, mice -> mouse. Stemming couldn't have done this\", end=\"\\n\\n\")\n",
    "#now each token i ll pass to he lemmatizer to see its reduced form\n",
    "#Lemmatize each word\n",
    "for word in input_str:\n",
    "    print(lemmatizer.lemmatize(word).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Text Preprocessing techniques discussed above on Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer=WordNetLemmatizer()\n",
    "for index,row in train.iterrows(): #taking he train data and iterating each row\n",
    "    filter_sentence = ''\n",
    "    \n",
    "    sentence = row['total']\n",
    "    sentence = re.sub(r'[^\\w\\s]','',sentence) \n",
    "    \n",
    "    words = nltk.word_tokenize(sentence) #tokenizing the sentence\n",
    "    \n",
    "    words = [w for w in words if not w in stop]  #removing the stopwords\n",
    "    \n",
    "    #after removing the stopwords, applying the WornNet Lemmatizer\n",
    "    for word in words:\n",
    "        filter_sentence = filter_sentence + ' ' + str(lemmatizer.lemmatize(word)).lower()\n",
    "        \n",
    "    # at the end, again putting the filter_sentence back into the training document at the same position    \n",
    "    train.loc[index,'total'] = filter_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_words(train['total'], 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df2 = pd.DataFrame(common_words, columns = ['total' , 'count'])\n",
    "df2.groupby('total').sum()['count'].sort_values(ascending=False).plot(\n",
    "kind='bar', title='Top 20 words in dataset after text-preprocessing',color=['salmon', 'tomato', 'darksalmon', 'coral', 'orangered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_bigram(train['total'], 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df4 = pd.DataFrame(common_words, columns = ['total' , 'count'])\n",
    "df4.groupby('total').sum()['count'].sort_values(ascending=False).plot(\n",
    "kind='bar', title='Top 20 bi-grams in dataset after text-preprocessing', color=['darkmagenta', 'orchid', 'mediumvioletred', 'deeppink', 'hotpink', 'palevioletred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Word Counts**\n",
    "\n",
    "Word counts also expected that the length of the table would be less influential, so only the length of the text was considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_number_histogram(textno, textye):\n",
    "    \n",
    "    \"\"\"A function for comparing word counts\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), sharey=True)\n",
    "    sns.distplot(textno.str.split().map(lambda x: len(x)), ax=axes[0], color='#e74c3c')\n",
    "    sns.distplot(textye.str.split().map(lambda x: len(x)), ax=axes[1], color='#e74c3c')\n",
    "    \n",
    "    axes[0].set_xlabel('Word Count')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Reliable')\n",
    "    axes[1].set_xlabel('Word Count')\n",
    "    axes[1].set_title('Unreliable')\n",
    "    \n",
    "    fig.suptitle('Fake News', fontsize=24, va='baseline')\n",
    "    \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_number_histogram(train[train['label'] == 0]['total'],\n",
    "                           train[train['label'] == 1]['total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unreliable has fewer words than Reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_len_histogram(textno, textye):\n",
    "    \n",
    "    \"\"\"A function for comparing average word length\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), sharey=True)\n",
    "    sns.distplot(textno.str.split().apply(lambda x: [len(i) for i in x]).map(\n",
    "        lambda x: np.mean(x)),\n",
    "                 ax=axes[0], color='#e74c3c')\n",
    "    sns.distplot(textye.str.split().apply(lambda x: [len(i) for i in x]).map(\n",
    "        lambda x: np.mean(x)),\n",
    "                 ax=axes[1], color='#e74c3c')\n",
    "    \n",
    "    axes[0].set_xlabel('Word Length')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Reliable')\n",
    "    axes[1].set_xlabel('Word Length')\n",
    "    axes[1].set_title('Unreliable')\n",
    "    \n",
    "    fig.suptitle('Mean Word Lengths', fontsize=24, va='baseline')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_len_histogram(train[train['label'] == 0]['total'],\n",
    "                        train[train['label'] == 1]['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_text = [\n",
    "\n",
    "    train[train['label'] == 0]['total'],\n",
    "    train[train['label'] == 1]['total']\n",
    "]\n",
    "\n",
    "lis_title = [\n",
    "    train[train['label'] == 0]['total'],\n",
    "    train[train['label'] == 1]['total']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, j in zip(lis_text, axes):\n",
    "    try:\n",
    "        new = i.str.split()\n",
    "        new = new.values.tolist()\n",
    "        corpus = [word.lower() for i in new for word in i]\n",
    "        dic = defaultdict(int)\n",
    "        for word in corpus:\n",
    "            if word in stop:\n",
    "                dic[word] += 1\n",
    "     #   print(dic)\n",
    "        top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "        #   print(top)\n",
    "        x, y = zip(*top)\n",
    "        df = pd.DataFrame([x, y]).T\n",
    "        df = df.rename(columns={0: 'Stopword', 1: 'Count'})\n",
    "        sns.barplot(x='Count', y='Stopword', data=df, palette='plasma', ax=j)\n",
    "        plt.tight_layout()\n",
    "    except:\n",
    "        plt.close()\n",
    "        print('No stopwords left in texts.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying most common words.\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, j in zip(lis_text, axes):\n",
    "\n",
    "    new = i.str.split()\n",
    "    new = new.values.tolist()\n",
    "    corpus = [word for i in new for word in i]\n",
    "\n",
    "    counter = Counter(corpus)\n",
    "    most = counter.most_common()\n",
    "    x, y = [], []\n",
    "    for word, count in most[:30]:\n",
    "        if (word not in stop):\n",
    "            x.append(word)\n",
    "            y.append(count)\n",
    "\n",
    "\n",
    "    sns.barplot(x=y, y=x, palette='plasma', ax=j)\n",
    "print(x, y)\n",
    "axes[0].set_title('Reliable')\n",
    "axes[1].set_title('Unreliable')\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[0].set_ylabel('Word')\n",
    "axes[1].set_xlabel('Count')\n",
    "axes[1].set_ylabel('Word')\n",
    "\n",
    "fig.suptitle('Most Common Unigrams in Text', fontsize=24, va='baseline')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, j in zip(lis_title, axes):\n",
    "\n",
    "    new = i.str.split()\n",
    "    new = new.values.tolist()\n",
    "    corpus = [word for i in new for word in i]\n",
    "\n",
    "    counter = Counter(corpus)\n",
    "    most = counter.most_common()\n",
    "    x, y = [], []\n",
    "    for word, count in most[:30]:\n",
    "        if (word not in stop):\n",
    "            x.append(word)\n",
    "            y.append(count)\n",
    "\n",
    "\n",
    "    sns.barplot(x=y, y=x, palette='plasma', ax=j)\n",
    "print(x, y)\n",
    "axes[0].set_title('Reliable')\n",
    "axes[1].set_title('Unreliable')\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[0].set_ylabel('Word')\n",
    "axes[1].set_xlabel('Count')\n",
    "axes[1].set_ylabel('Word')\n",
    "\n",
    "fig.suptitle('Most Common Unigrams in Title', fontsize=24, va='baseline')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most Common Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def ngrams(n, title, lis_type):\n",
    "    \"\"\"A Function to plot most common ngrams\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    axes = axes.flatten()\n",
    "    for i, j in zip(lis_type, axes):\n",
    "\n",
    "        new = i.str.split()\n",
    "        new = new.values.tolist()\n",
    "        corpus = [word for i in new for word in i]\n",
    "\n",
    "        def _get_top_ngram(corpus, n=None):\n",
    "            #getting top ngrams\n",
    "            vec = CountVectorizer(ngram_range=(n, n),\n",
    "                                  max_df=0.9,\n",
    "                                  stop_words='english').fit(corpus)\n",
    "            bag_of_words = vec.transform(corpus)\n",
    "            sum_words = bag_of_words.sum(axis=0)\n",
    "            words_freq = [(word, sum_words[0, idx])\n",
    "                          for word, idx in vec.vocabulary_.items()]\n",
    "            words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "            return words_freq[:15]\n",
    "\n",
    "        top_n_bigrams = _get_top_ngram(i, n)[:15]\n",
    "        x, y = map(list, zip(*top_n_bigrams))\n",
    "        sns.barplot(x=y, y=x, palette='plasma', ax=j)\n",
    "        \n",
    "        axes[0].set_title('Reliable')\n",
    "        axes[1].set_title('Unreliable')\n",
    "        axes[0].set_xlabel('Count')\n",
    "        axes[0].set_ylabel('Words')\n",
    "        axes[1].set_xlabel('Count')\n",
    "        axes[1].set_ylabel('Words')\n",
    "        fig.suptitle(title, fontsize=24, va='baseline')\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams(2, 'Most Common Bigrams', lis_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams(2, 'Most Common Bigrams', lis_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the total words present in the dataset\n",
    "list_of_words = []\n",
    "for i in train.total:\n",
    "    for j in i:\n",
    "        list_of_words.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the total number of unique words\n",
    "total_words = len(list(set(list_of_words)))\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe information\n",
    "train.info()\n",
    "# check for null values\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We require only the following 2 columns now:\n",
    "- 'total' --- holds the preprocessed text\n",
    "- 'label' --- holds the predictions from where the machine will learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['total','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()\n",
    "#column 'total' has the preprocessed text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is cleaned, we can use CountVectorizer to convert our data into the format in which sklearn requires\n",
    "\n",
    "1. CountVectorizer helps in <u>Feature Extraction</u> \n",
    "2. It convert a collection of text documents to a matrix of token counts \n",
    "\n",
    "CountVectorizer produces a sparse matrix.\n",
    "CountVectorizer can also take care of stop words. There is an option in count vectorizer(stop_words) which takes list of stop words and can do the work for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "train_set = {\"the sky is blue\", \"the sun is bright\"}\n",
    "count_vec = CountVectorizer(max_features = 3)\n",
    "a = count_vec.fit_transform(train_set)\n",
    "a.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying NLP Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['total']\n",
    "Y_train = train['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words / CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the corpus holds some sentences\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# in sklearn, we can import Bag of Words model through CountVectorizer. This works pretty much like Bag of words\n",
    "# it is modified version of Bag of Words. It replaces the vector, instead of 1, w the frequency...\n",
    "\n",
    "# we r making the object, vectorizer, of class/module CountVectorizer()\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"The above words are the unique words and consists of the feature set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The below matrix will show the frequency of the features in the feature set\", end=\"\\n\\n\")\n",
    "X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-iDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(features, max_features):\n",
    "    vectorizer = TfidfVectorizer( stop_words='english', #it will remove the englsh stopwords\n",
    "                            decode_error='strict',\n",
    "                            analyzer='word',\n",
    "                            ngram_range=(1, 2), #single_words or 2words(bi-grams)\n",
    "                            max_features=max_features\n",
    "                            #max_df=0.5 # Verwendet im ML-Kurs unter Preprocessing                   \n",
    "                            )\n",
    "    feature_vec = vectorizer.fit_transform(features)\n",
    "    return feature_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features = vectorize_text(['hello how are you doing','hi i am doing fine'],30) \n",
    "# 30 here is the number of max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"creates some weight for all these words: \", end='\\n\\n')\n",
    "tfidf_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Feature Extraction using count vectorization and tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processed text is in X_train and the labels in Y_train as follows (done above):\n",
    "\n",
    "X_train = train['total']\n",
    "Y_train = train['label']\n",
    "\n",
    "\n",
    "Taking a CountVectorizer, whatever the output of the CountVectorizer is, applying TF-IDF transformer on top of that. Output of CountVectorizer will be some vectors with their Term Frequency. TF-IDF works better with CountVectorizer. So we are using both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Feature extraction using count vectorization and tfidf.\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit_transform(X_train)\n",
    "freq_term_matrix = count_vectorizer.transform(X_train)\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(freq_term_matrix)\n",
    "tf_idf_matrix = tfidf.fit_transform(freq_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_vectorizer.vocabulary_.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"10 feature names are:\", end = '\\n\\n')\n",
    "count_vectorizer.get_feature_names()[9000:9010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20800 samples and 220387 features were generated so each text sample of holding this number(220387) of length.\n",
    "Now, text data has been converted into numbers using TF-IDF transformer and CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply transform on the test data also because once we build the models, we need to predict for the test data and split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split in samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_matrix, Y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that is, for testing we now have 5200 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training data - X_train.shape- 15600 -- these are those samples for which we have the labels also.\n",
    "Now we will train the model on these 15600 samples and the model is able to give some predictions after that.\n",
    "\n",
    "We have to test on new(test data) data because the model may always give correct answer to the training data possibly maybe because of OVERFITTING\n",
    "\n",
    "Now these 5200 X_test samples, we know the actual output (or, label) for these samples\n",
    "And, omce our model is trained, we will test it with these 5200 X_test samples and we will get some predictions. \n",
    "Now we compare the prediction with the actual labels, then we know that how our model has performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "model = PassiveAggressiveClassifier(max_iter=10000, random_state=1,tol=1e-3).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_pa = model.predict(X_test)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test,y_pred_pa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_pa)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.set(font_scale=3.0) # Adjust to fit\n",
    "sns.heatmap(cm, annot=True, ax=ax, cmap=\"Blues\", fmt=\"g\");  \n",
    "\n",
    "# Labels, title and ticks\n",
    "label_font = {'size':'18'}  # Adjust to fit\n",
    "ax.set_xlabel('Predicted labels', fontdict=label_font);\n",
    "ax.set_ylabel('True labels', fontdict=label_font);\n",
    "\n",
    "title_font = {'size':'21'}  # Adjust to fit\n",
    "ax.set_title('Confusion Matrix for Passive Aggressive Classifier on Fake News Dataset', fontdict=title_font);\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)  # Adjust to fit\n",
    "ax.xaxis.set_ticklabels(['Real', 'Fake']);\n",
    "ax.yaxis.set_ticklabels(['Real', 'Fake']);\n",
    "plt.show()\n",
    "plt.savefig('PassiveAgressive.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ann= clf.predict(X_test)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,pred_ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred_ann)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.set(font_scale=3.0) # Adjust to fit\n",
    "sns.heatmap(cm, annot=True, ax=ax, cmap=\"PiYG\", fmt=\"g\");  \n",
    "\n",
    "# Labels, title and ticks\n",
    "label_font = {'size':'18'}  # Adjust to fit\n",
    "ax.set_xlabel('Predicted labels', fontdict=label_font);\n",
    "ax.set_ylabel('True labels', fontdict=label_font);\n",
    "\n",
    "title_font = {'size':'21'}  # Adjust to fit\n",
    "ax.set_title('Confusion Matrix for Multi Layer Perceptron on Fake News Dataset', fontdict=title_font);\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)  # Adjust to fit\n",
    "ax.xaxis.set_ticklabels(['Real', 'Fake']);\n",
    "ax.yaxis.set_ticklabels(['Real', 'Fake']);\n",
    "plt.show()\n",
    "plt.savefig('MLP.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lear abt Logistic Regression at 1 hr, 22 mins\n",
    "(C=1e5) is a regularisation parameter. Regularisation is used to avoid the overfitting when we have a lot of features, like here for each sample(15600), we have (220387) features. A regularisation parameter should be as low as possible so that we can remove the overfitting, these things can be fine tuned again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e5, random_state=110, max_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = logreg.predict(X_test)\n",
    "\n",
    "print('Accuracy of Logistic Regression on test set: {:.5f}'\n",
    "     .format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.set(font_scale=3.0) # Adjust to fit\n",
    "sns.heatmap(cm, annot=True, ax=ax, cmap=\"coolwarm\", fmt=\"g\");  \n",
    "\n",
    "# Labels, title and ticks\n",
    "label_font = {'size':'18'}  # Adjust to fit\n",
    "ax.set_xlabel('Predicted labels', fontdict=label_font);\n",
    "ax.set_ylabel('True labels', fontdict=label_font);\n",
    "\n",
    "title_font = {'size':'21'}  # Adjust to fit\n",
    "ax.set_title('Confusion Matrix for Logistic Regression on Fake News Dataset', fontdict=title_font);\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)  # Adjust to fit\n",
    "ax.xaxis.set_ticklabels(['Real', 'Fake']);\n",
    "ax.yaxis.set_ticklabels(['Real', 'Fake']);\n",
    "plt.show()\n",
    "plt.savefig('Logistic.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes Classifier works on Conditional Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB = MultinomialNB()\n",
    "NB.fit(X_train, y_train)\n",
    "pred_NB = NB.predict(X_test)\n",
    "print('Accuracy of MultinomialNB classifier on test set: {:.2f}'\n",
    "     .format(NB.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test,pred_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred_NB)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.set(font_scale=3.0) # Adjust to fit\n",
    "sns.heatmap(cm, annot=True, ax=ax, cmap=\"bwr\", fmt=\"g\");  \n",
    "\n",
    "# Labels, title and ticks\n",
    "label_font = {'size':'18'}  # Adjust to fit\n",
    "ax.set_xlabel('Predicted labels', fontdict=label_font);\n",
    "ax.set_ylabel('True labels', fontdict=label_font);\n",
    "\n",
    "title_font = {'size':'21'}  # Adjust to fit\n",
    "ax.set_title('Confusion Matrix for Multinomial Naive Bayes Classifier on Fake News Dataset', fontdict=title_font);\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)  # Adjust to fit\n",
    "ax.xaxis.set_ticklabels(['Real', 'Fake']);\n",
    "ax.yaxis.set_ticklabels(['Real', 'Fake']);\n",
    "plt.show()\n",
    "plt.savefig('MultinomialNB.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(X_train, y_train)\n",
    "pred_dt = DT.predict(X_test)\n",
    "DT.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred_dt)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.set(font_scale=3.0) # Adjust to fit\n",
    "sns.heatmap(cm, annot=True, ax=ax, cmap=\"seismic\", fmt=\"g\");  \n",
    "\n",
    "# Labels, title and ticks\n",
    "label_font = {'size':'18'}  # Adjust to fit\n",
    "ax.set_xlabel('Predicted labels', fontdict=label_font);\n",
    "ax.set_ylabel('True labels', fontdict=label_font);\n",
    "\n",
    "title_font = {'size':'21'}  # Adjust to fit\n",
    "ax.set_title('Confusion Matrix for Decision Tree Classifier on Fake News Dataset', fontdict=title_font);\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)  # Adjust to fit\n",
    "ax.xaxis.set_ticklabels(['Real', 'Fake']);\n",
    "ax.yaxis.set_ticklabels(['Real', 'Fake']);\n",
    "plt.show()\n",
    "plt.savefig('DT.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBC = GradientBoostingClassifier(random_state=0)\n",
    "GBC.fit(X_train, y_train)\n",
    "pred_gbc = GBC.predict(X_test)\n",
    "GBC.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_gbc))\n",
    "print(GBC.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred_gbc)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.set(font_scale=3.0) # Adjust to fit\n",
    "sns.heatmap(cm, annot=True, ax=ax, cmap=\"RdGy\", fmt=\"g\");  \n",
    "\n",
    "# Labels, title and ticks\n",
    "label_font = {'size':'18'}  # Adjust to fit\n",
    "ax.set_xlabel('Predicted labels', fontdict=label_font);\n",
    "ax.set_ylabel('True labels', fontdict=label_font);\n",
    "\n",
    "title_font = {'size':'21'}  # Adjust to fit\n",
    "ax.set_title('Confusion Matrix for Gradient Boosting Classifier on Fake News Dataset', fontdict=title_font);\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)  # Adjust to fit\n",
    "ax.xaxis.set_ticklabels(['Real', 'Fake']);\n",
    "ax.yaxis.set_ticklabels(['Real', 'Fake']);\n",
    "plt.show()\n",
    "plt.savefig('GradientBoostingClassifier.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RFC = RandomForestClassifier(n_estimators=50, criterion=\"entropy\")\n",
    "RFC.fit(X_train, y_train)\n",
    "pred_RFC = RFC.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RFC.score(X_test, y_test))\n",
    "print(classification_report(y_test, pred_RFC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred_RFC)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.set(font_scale=3.0) # Adjust to fit\n",
    "sns.heatmap(cm, annot=True, ax=ax, cmap=\"PuOr\", fmt=\"g\");  \n",
    "\n",
    "# Labels, title and ticks\n",
    "label_font = {'size':'18'}  # Adjust to fit\n",
    "ax.set_xlabel('Predicted labels', fontdict=label_font);\n",
    "ax.set_ylabel('True labels', fontdict=label_font);\n",
    "\n",
    "title_font = {'size':'21'}  # Adjust to fit\n",
    "ax.set_title('Confusion Matrix for Random Forest Classifier on Fake News Dataset', fontdict=title_font);\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)  # Adjust to fit\n",
    "ax.xaxis.set_ticklabels(['Real', 'Fake']);\n",
    "ax.yaxis.set_ticklabels(['Real', 'Fake']);\n",
    "plt.show()\n",
    "plt.savefig('RandomForestClassifier.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "pred_knn = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn.score(X_test, y_test))\n",
    "print(classification_report(y_test, pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred_knn)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.set(font_scale=3.0) # Adjust to fit\n",
    "sns.heatmap(cm, annot=True, ax=ax, cmap=\"Pastel1\", fmt=\"g\");  \n",
    "\n",
    "# Labels, title and ticks\n",
    "label_font = {'size':'18'}  # Adjust to fit\n",
    "ax.set_xlabel('Predicted labels', fontdict=label_font);\n",
    "ax.set_ylabel('True labels', fontdict=label_font);\n",
    "\n",
    "title_font = {'size':'21'}  # Adjust to fit\n",
    "ax.set_title('Confusion Matrix for K-Nearest-Neighbours Classifier on Fake News Dataset', fontdict=title_font);\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)  # Adjust to fit\n",
    "ax.xaxis.set_ticklabels(['Real', 'Fake']);\n",
    "ax.yaxis.set_ticklabels(['Real', 'Fake']);\n",
    "plt.show()\n",
    "plt.savefig('K-Nearest-Neighbours1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = np.arange(1, 15)\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, k in enumerate(neighbors):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('k-NN: Analysis of varying Number of Neighbors')\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train, y_train)\n",
    "pred_knn = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn.score(X_test, y_test))\n",
    "print(classification_report(y_test, pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred_knn)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.set(font_scale=3.0) # Adjust to fit\n",
    "sns.heatmap(cm, annot=True, ax=ax, cmap=\"Pastel2\", fmt=\"g\");  \n",
    "\n",
    "# Labels, title and ticks\n",
    "label_font = {'size':'18'}  # Adjust to fit\n",
    "ax.set_xlabel('Predicted labels', fontdict=label_font);\n",
    "ax.set_ylabel('True labels', fontdict=label_font);\n",
    "\n",
    "title_font = {'size':'21'}  # Adjust to fit\n",
    "ax.set_title('Confusion Matrix for K-Nearest-Neighbours Classifier on Fake News Dataset', fontdict=title_font);\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)  # Adjust to fit\n",
    "ax.xaxis.set_ticklabels(['Real', 'Fake']);\n",
    "ax.yaxis.set_ticklabels(['Real', 'Fake']);\n",
    "plt.show()\n",
    "plt.savefig('K-Nearest-Neighbours2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM -Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics\n",
    "\n",
    "svm_ = svm.SVC(kernel=\"linear\")\n",
    "svm_.fit(X_train, y_train)\n",
    "pred_svm = svm_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svm_.score(X_test, y_test))\n",
    "print(classification_report(y_test, pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred_svm)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.set(font_scale=3.0) # Adjust to fit\n",
    "sns.heatmap(cm, annot=True, ax=ax, cmap=\"Paired\", fmt=\"g\");  \n",
    "\n",
    "# Labels, title and ticks\n",
    "label_font = {'size':'18'}  # Adjust to fit\n",
    "ax.set_xlabel('Predicted labels', fontdict=label_font);\n",
    "ax.set_ylabel('True labels', fontdict=label_font);\n",
    "\n",
    "title_font = {'size':'21'}  # Adjust to fit\n",
    "ax.set_title('Confusion Matrix for SVM Classifier on Fake News Dataset', fontdict=title_font);\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)  # Adjust to fit\n",
    "ax.xaxis.set_ticklabels(['Real', 'Fake']);\n",
    "ax.yaxis.set_ticklabels(['Real', 'Fake']);\n",
    "plt.show()\n",
    "plt.savefig('SVM.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_adaBoost = clf.predict(X_test)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,pred_adaBoost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred_adaBoost)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.set(font_scale=3.0) # Adjust to fit\n",
    "sns.heatmap(cm, annot=True, ax=ax, cmap=\"rainbow\", fmt=\"g\");  \n",
    "\n",
    "# Labels, title and ticks\n",
    "label_font = {'size':'18'}  # Adjust to fit\n",
    "ax.set_xlabel('Predicted labels', fontdict=label_font);\n",
    "ax.set_ylabel('True labels', fontdict=label_font);\n",
    "\n",
    "title_font = {'size':'21'}  # Adjust to fit\n",
    "ax.set_title('Confusion Matrix for AdaBoost Classifier on Fake News Dataset', fontdict=title_font);\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)  # Adjust to fit\n",
    "ax.xaxis.set_ticklabels(['Real', 'Fake']);\n",
    "ax.yaxis.set_ticklabels(['Real', 'Fake']);\n",
    "plt.show()\n",
    "plt.savefig('AdaBoost.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_XGBoost = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test,pred_XGBoost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred_XGBoost)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.set(font_scale=3.0) # Adjust to fit\n",
    "sns.heatmap(cm, annot=True, ax=ax, cmap=\"gist_ncar\", fmt=\"g\");  \n",
    "\n",
    "# Labels, title and ticks\n",
    "label_font = {'size':'18'}  # Adjust to fit\n",
    "ax.set_xlabel('Predicted labels', fontdict=label_font);\n",
    "ax.set_ylabel('True labels', fontdict=label_font);\n",
    "\n",
    "title_font = {'size':'21'}  # Adjust to fit\n",
    "ax.set_title('Confusion Matrix for XGBoost Classifier on Fake News Dataset', fontdict=title_font);\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)  # Adjust to fit\n",
    "ax.xaxis.set_ticklabels(['Real', 'Fake']);\n",
    "ax.yaxis.set_ticklabels(['Real', 'Fake']);\n",
    "plt.show()\n",
    "plt.savefig('XGBoost.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
